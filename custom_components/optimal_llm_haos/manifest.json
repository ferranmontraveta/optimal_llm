{
  "domain": "optimal_llm_haos",
  "name": "Optimal LLM HAOS",
  "after_dependencies": ["assist_pipeline", "intent"],
  "codeowners": [],
  "config_flow": true,
  "dependencies": ["conversation"],
  "documentation": "https://github.com/optimal_llm_haos",
  "integration_type": "service",
  "iot_class": "local_polling",
  "issue_tracker": "https://github.com/optimal_llm_haos/issues",
  "requirements": ["ollama>=0.4.0"],
  "version": "0.1.0"
}